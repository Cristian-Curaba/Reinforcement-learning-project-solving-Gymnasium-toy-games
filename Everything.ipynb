{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install gymnasium[toy-text]\n",
    "# !pip install tqdm #just for progress bar\n",
    "# !pip install icecream #just for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gymnasium v0.29.0\n",
      "Python v3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "from icecream import ic #Just for debugging\n",
    "\n",
    "#check version\n",
    "import gymnasium as gym\n",
    "print(f'Gymnasium v{gym.__version__}') #Gymnasium v0.29.0\n",
    "import sys; print(f'Python v{sys.version}') #Python v3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#create env\n",
    "env=gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode='rgb_array') #specific map\n",
    "#env=gym.make('Blackjack-v1',render_mode='rgb_array')\n",
    "#env=gym.make('CliffWalking-v0',render_mode='rgb_array')\n",
    "#env=gym.make('Taxi-v3',render_mode='rgb_array')\n",
    "\n",
    "done=False\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Just some manual tests\n",
    "# import matplotlib.pyplot as plt\n",
    "# if terminated or truncated:\n",
    "#     env.reset()\n",
    "# obs, reward, terminated, truncated, info =env.step(1)\n",
    "# print(obs)\n",
    "# print(reward)\n",
    "\n",
    "# img=env.render()\n",
    "# fig, ax=plt.subplots()\n",
    "# ax.imshow(img)\n",
    "# ax.set_title('Game')\n",
    "# ax.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TD_zero_for_Qvalue:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float=0.01,\n",
    "        initial_epsilon: float=1,\n",
    "        epsilon_decay: float=0.0001,\n",
    "        final_epsilon: float=0.0001,\n",
    "        discount_factor: float=0.95,\n",
    "        n_episodes: int=10000,\n",
    "        env_stat: RecordEpisodeStatistics=env,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env_stat.action_space.n))\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.n_episodes = n_episodes\n",
    "        self.env_stat = env_stat\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env_stat.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(self, obs, action, reward, terminated, next_obs):\n",
    "        \"\"\"Updates the Q-values with Temporal Difference\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        self.q_values[obs][action] = self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def linear_decay(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "    def exponential_decay(self):\n",
    "        self.epsilon = self.epsilon * self.epsilon_decay\n",
    "\n",
    "    def train(self):\n",
    "        for episode in tqdm(range(self.n_episodes)):\n",
    "            obs, info = self.env_stat.reset()\n",
    "            done = False\n",
    "\n",
    "            # Play one episode\n",
    "            while not done:\n",
    "                action = self.get_action(obs)\n",
    "                next_obs, reward, terminated, truncated, info = self.env_stat.step(action)\n",
    "\n",
    "                # Update agent policy at each step\n",
    "                self.update(obs, action, reward, terminated, next_obs)\n",
    "                done = terminated or truncated\n",
    "                obs = next_obs\n",
    "\n",
    "                if info.get('TimeLimit.truncated', False):\n",
    "                    done = True  # Terminate the episode if max episode steps are reached\n",
    "\n",
    "            self.linear_decay()\n",
    "\n",
    "    def plot(self, rolling_length=100):\n",
    "        fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "        axs[0].set_title(\"Episode rewards\")\n",
    "        reward_moving_average = (\n",
    "            np.convolve(\n",
    "                np.array(self.env_stat.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "            )\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "        axs[1].set_title(\"Episode lengths\")\n",
    "        length_moving_average = (\n",
    "            np.convolve(\n",
    "                np.array(self.env_stat.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "            )\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "        axs[2].set_title(\"Training Error\")\n",
    "        training_error_moving_average = (\n",
    "            np.convolve(np.array(self.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "        title = \"TD method: learning_rate=\" + str(self.lr) + \", discount_factor=\" + str(self.discount_factor)\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        title2=\"TD_\"+ str(self.lr)+ \"_\"+ str(self.discount_factor)+ \".png\"\n",
    "        plt.savefig(title2)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expected_SARSA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float=0.01,\n",
    "        initial_epsilon: float=1,\n",
    "        epsilon_decay: float=0.0001,\n",
    "        final_epsilon: float=0.0001,\n",
    "        discount_factor: float = 0.95,\n",
    "        n_episodes: int=10000,\n",
    "        env_stat: RecordEpisodeStatistics=env,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values=defaultdict(lambda: np.zeros(env.action_space.n)) #everytime new statevalue added the lambda function is called to create a np vector of q values (initialized to zero)\n",
    "        self.lr = learning_rate #the bigger the more oscillations q_values\n",
    "        self.discount_factor = discount_factor \n",
    "        self.epsilon = initial_epsilon \n",
    "        self.epsilon_decay = epsilon_decay #linear decay until reaches final_epsilon\n",
    "        self.final_epsilon = final_epsilon #approximately zero \n",
    "        self.n_episodes= n_episodes\n",
    "        self.env_stat=env_stat\n",
    "        self.training_error=[] #temporale differences (when small, no further improvements)\n",
    "    def get_action(self, obs) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        if np.random.random()<self.epsilon:\n",
    "            return self.env_stat.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "    def update(self, obs, action, reward, next_obs,  terminated):\n",
    "        \"\"\"Updates the Q-values with EXPECTED-SARSA rule by probabilities given by the policy\"\"\"\n",
    "        expected_value = sum(\n",
    "         ((1 - self.epsilon + self.epsilon / self.env_stat.action_space.n) if a == int(np.argmax(self.q_values[next_obs])) else self.epsilon / self.env_stat.action_space.n)\n",
    "            * self.q_values[next_obs][a]\n",
    "                  for a in range(self.env_stat.action_space.n)\n",
    "                    ) / self.env_stat.action_space.n\n",
    "\n",
    "        self.q_values[obs][action]=self.q_values[obs][action] + self.lr*(reward+self.discount_factor*expected_value-self.q_values[obs][action])\n",
    "        self.training_error.append(reward+self.discount_factor*expected_value-self.q_values[obs][action]) \n",
    "    def linear_decay(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "    def exponential_decay(self): \n",
    "        self.epsilon= self.epsilon*self.epsilon_decay   \n",
    "    def train(self):\n",
    "        for episode in tqdm(range(self.n_episodes)):\n",
    "            obs, info = self.env_stat.reset()\n",
    "            done = False\n",
    "            action = self.get_action(obs)\n",
    "            # play one episode\n",
    "            while not done:\n",
    "                next_obs, reward, terminated, truncated, info = self.env_stat.step(action)\n",
    "                #update agent policy at each step\n",
    "                self.update(obs, action, reward, next_obs, terminated)\n",
    "                done= terminated or truncated\n",
    "                obs=next_obs\n",
    "                if info.get('TimeLimit.truncated', False):\n",
    "                    done = True  # Terminate the episode if max episode steps are reached\n",
    "            self.linear_decay()\n",
    "            #ic(self.epsilon)\n",
    "    def plot(self, rolling_length=100):\n",
    "        fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "        axs[0].set_title(\"Episode rewards\")\n",
    "        reward_moving_average = (\n",
    "            np.convolve(\n",
    "                np.array(self.env_stat.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "            )\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "        axs[1].set_title(\"Episode lengths\")\n",
    "        length_moving_average = (\n",
    "            np.convolve(\n",
    "                np.array(self.env_stat.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "            )\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "        axs[2].set_title(\"Training Error\")\n",
    "        training_error_moving_average = (\n",
    "            np.convolve(np.array(self.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "        title= \"Expected SARSA: learning_rate= \"+ str(self.lr) + \", discount_factor= \" + str(self.discount_factor)\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        title2=\"E_sarsa\"+ str(self.lr)+ \"_\"+ str(self.discount_factor)+ \".png\"\n",
    "        plt.savefig(title2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float=0.01,\n",
    "        initial_epsilon: float=1,\n",
    "        epsilon_decay: float=0.0001,\n",
    "        final_epsilon: float=0.0001,\n",
    "        discount_factor: float = 0.95,\n",
    "        n_episodes: int=10000,\n",
    "        env_stat: RecordEpisodeStatistics=env,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values=defaultdict(lambda: np.zeros(env.action_space.n)) #everytime new statevalue added the lambda function is called to create a np vector of q values (initialized to zero)\n",
    "        self.lr = learning_rate #the bigger the more oscillations q_values\n",
    "        self.discount_factor = discount_factor \n",
    "        self.epsilon = initial_epsilon \n",
    "        self.epsilon_decay = epsilon_decay #linear decay until reaches final_epsilon\n",
    "        self.final_epsilon = final_epsilon #approximately zero \n",
    "        self.n_episodes= n_episodes\n",
    "        self.env_stat=env_stat\n",
    "        self.training_error=[] #temporale differences (when small, no further improvements)\n",
    "    def get_action(self, obs) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        if np.random.random()<self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "    def update(self, obs, action, reward, next_obs, next_action,  terminated):\n",
    "        \"\"\"Updates the Q-values with SARSA\"\"\"\n",
    "\n",
    "        self.q_values[obs][action]=self.q_values[obs][action] + self.lr*(reward+self.discount_factor*self.q_values[next_obs][next_action]-self.q_values[obs][action])\n",
    "        self.training_error.append(reward+self.discount_factor*self.q_values[next_obs][next_action]-self.q_values[obs][action]) \n",
    "    def linear_decay(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "    def exponential_decay(self): \n",
    "        self.epsilon= self.epsilon*self.epsilon_decay\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        for episode in tqdm(range(self.n_episodes)):\n",
    "            obs, info = self.env_stat.reset()\n",
    "            done = False\n",
    "            action = self.get_action(obs)\n",
    "            # play one episode\n",
    "            while not done:\n",
    "                next_obs, reward, terminated, truncated, info = self.env_stat.step(action)\n",
    "                next_action = self.get_action(next_obs)\n",
    "                #update agent policy at each step\n",
    "                self.update(obs, action, reward, next_obs, next_action, terminated)\n",
    "                done= terminated or truncated\n",
    "                obs=next_obs\n",
    "                action=next_action\n",
    "                if info.get('TimeLimit.truncated', False):\n",
    "                    done = True  # Terminate the episode if max episode steps are reached\n",
    "            self.linear_decay()\n",
    "            #ic(self.epsilon)\n",
    "\n",
    "    def plot(self, rolling_length=100):\n",
    "        fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "        axs[0].set_title(\"Episode rewards\")\n",
    "        reward_moving_average = (\n",
    "            np.convolve(\n",
    "                np.array(self.env_stat.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "            )\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "        axs[1].set_title(\"Episode lengths\")\n",
    "        length_moving_average = (\n",
    "            np.convolve(\n",
    "                np.array(self.env_stat.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "            )\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "        axs[2].set_title(\"Training Error\")\n",
    "        training_error_moving_average = (\n",
    "            np.convolve(np.array(self.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "        title= \"SARSA: learning_rate= \"+ str(self.lr) + \", discount_factor= \" + str(self.discount_factor)\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        title2=\"SARSA_\"+ str(self.lr)+ \"_\"+ str(self.discount_factor)+ \".png\"\n",
    "        plt.savefig(title2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class constant_learning_rate_MC:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float=0.99,\n",
    "        initial_epsilon: float=0.0001,\n",
    "        epsilon_decay: float=0.0001,\n",
    "        final_epsilon: float=0.0001,\n",
    "        discount_factor: float = 0.95,\n",
    "        n_episodes: int=3000,\n",
    "        env_stat: RecordEpisodeStatistics=env,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values=defaultdict(lambda: np.zeros(env.action_space.n)) #everytime new statevalue added the lambda function is called to create a np vector of q values (initialized to zero)\n",
    "        self.lr = learning_rate #the bigger the more oscillations q_values\n",
    "        self.discount_factor = discount_factor \n",
    "        self.epsilon = initial_epsilon \n",
    "        self.epsilon_decay = epsilon_decay #linear decay until reaches final_epsilon\n",
    "        self.final_epsilon = final_epsilon #approximately zero \n",
    "        self.n_episodes= n_episodes\n",
    "        self.obs_traj=[]\n",
    "        self.action_traj=[]\n",
    "        self.reward_traj=[]\n",
    "        self.env_stat=env_stat\n",
    "        self.training_error=[] #temporale differences (when small, no further improvements expected)\n",
    "\n",
    "    def get_action(self, obs) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        if np.random.random()<self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def linear_decay(self):\n",
    "        self.epsilon=max(self.final_epsilon, self.epsilon-self.epsilon_decay)\n",
    "    def exponential_decay(self): \n",
    "        self.epsilon= self.epsilon*self.epsilon_decay\n",
    "\n",
    "\n",
    "    def update_traj(self, obs, action, reward, terminated):\n",
    "        \"\"\"Append new obs, action, reward. When terminated, update policy and trash the episode\"\"\"\n",
    "        self.action_traj.append(action)\n",
    "        self.reward_traj.append(reward)\n",
    "        self.obs_traj.append(obs)\n",
    "        #ic(obs)\n",
    "        if terminated:\n",
    "            self.update_policy()           \n",
    "            self.obs_traj=[]\n",
    "            self.action_traj=[]\n",
    "            self.reward_traj=[] \n",
    "    def update_policy(self):\n",
    "        \"\"\"Updates the Q-values from last episode using the following:\n",
    "                Q(S_t, A_t)<- Q(S_t, A_t) + learning_rate*(G_t-Q(S_t,A_t)),\n",
    "                where G_t is the return at time-step t.\n",
    "        \"\"\"\n",
    "        G=0\n",
    "        sum=0\n",
    "        visited=[]\n",
    "        for obs, action, reward in reversed(list(zip(self.obs_traj, self.action_traj, self.reward_traj))):\n",
    "            G= self.discount_factor*G + reward\n",
    "            #Update once per episode\n",
    "            if (obs, action) not in visited:\n",
    "                self.q_values[obs][action]= self.q_values[obs][action] + self.lr*(G - self.q_values[obs][action])\n",
    "                visited.append((obs,action))\n",
    "                sum+=abs(G - self.q_values[obs][action])\n",
    "            #ic(visited)\n",
    "        #if any(self.q_values[15]):\n",
    "        #    ic(self.q_values[15])\n",
    "        self.training_error.append(sum/len(visited))\n",
    "    \n",
    "    def train(self):\n",
    "        for episode in tqdm(range(self.n_episodes)):\n",
    "            obs, info = self.env_stat.reset()\n",
    "            done = False\n",
    "            # play one episode\n",
    "            while not done:\n",
    "                action = self.get_action(obs)\n",
    "                next_obs, reward, terminated, truncated, info = self.env_stat.step(action)\n",
    "\n",
    "                #update_traj at each step and update policy at the end\n",
    "                self.update_traj(next_obs, action, reward, terminated)\n",
    "                done= terminated or truncated\n",
    "                obs=next_obs\n",
    "                if info.get('TimeLimit.truncated', False):\n",
    "                    done = True  # Terminate the episode if max episode steps are reached\n",
    "            self.linear_decay()\n",
    "            #ic(self.epsilon)\n",
    "            #ic(self.q_values[0])\n",
    "   \n",
    "    def plot(self, rolling_length=100):\n",
    "        fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "        axs[0].set_title(\"Episode rewards\")\n",
    "        reward_moving_average = (\n",
    "            np.convolve(\n",
    "                np.array(self.env_stat.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "            )\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "\n",
    "        axs[1].set_title(\"Episode lengths\")\n",
    "        length_moving_average = (\n",
    "            np.convolve(\n",
    "                np.array(self.env_stat.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "            )\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "\n",
    "        axs[2].set_title(\"Training Error\")\n",
    "        training_error_moving_average = (\n",
    "            np.convolve(np.array(self.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "            / rolling_length\n",
    "        )\n",
    "        axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "        \n",
    "        title= \"Constant Learning Rate Montecarlo: learning_rate= \"+ str(self.lr)\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        title2=\"MC_\"+ str(self.lr)+\".png\"\n",
    "        plt.savefig(title2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env=gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode='rgb_array') #specific map\n",
    "#env=gym.make('Blackjack-v1',render_mode='rgb_array')\n",
    "env=gym.make('CliffWalking-v0',render_mode='rgb_array')\n",
    "#env=gym.make('Taxi-v3',render_mode='rgb_array')\n",
    "\n",
    "done=False\n",
    "observation, info = env.reset()\n",
    "\n",
    "\n",
    "#hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 50000\n",
    "start_epsilon = 1.0\n",
    "final_epsilon = 0.001\n",
    "epsilon_decay = start_epsilon/n_episodes  # reduce the exploration over time linearly\n",
    "discount_factor=0.99\n",
    "#epsilon_decay= final_epsilon**(1/n_episodes) #with n_episodes updates epsilon reaches 0.000001\n",
    "\n",
    "\n",
    "# Define the maximum number of steps per episode\n",
    "max_episode_steps = 200\n",
    "\n",
    "from gym.wrappers import TimeLimit\n",
    "# Wrap the environment with the TimeLimit wrapper\n",
    "env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "\n",
    "env_stat = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = [0.5, 0.8, 0.9, 0.95, 0.99]\n",
    "# discount_factors = [0.8, 0.95, 0.99]\n",
    "# for learning_rate in learning_rates:\n",
    "#     for discount_factor in discount_factors:\n",
    "#         agent_TD=TD_zero_for_Qvalue(\n",
    "#     n_episodes=n_episodes,\n",
    "#     learning_rate=learning_rate,\n",
    "#     discount_factor=discount_factor,\n",
    "#     initial_epsilon=start_epsilon,\n",
    "#     epsilon_decay=epsilon_decay,\n",
    "#     final_epsilon=final_epsilon,\n",
    "#     env_stat=env_stat,)\n",
    "#         agent_sarsa=SARSA(\n",
    "#     n_episodes=n_episodes,\n",
    "#     learning_rate=learning_rate,\n",
    "#     discount_factor=discount_factor,\n",
    "#     initial_epsilon=start_epsilon,\n",
    "#     epsilon_decay=epsilon_decay,\n",
    "#     final_epsilon=final_epsilon,\n",
    "#     env_stat=env_stat,)\n",
    "#         agent_E_sarsa=Expected_SARSA(\n",
    "#     n_episodes=n_episodes,\n",
    "#     learning_rate=learning_rate,\n",
    "#     discount_factor=discount_factor,\n",
    "#     initial_epsilon=start_epsilon,\n",
    "#     epsilon_decay=epsilon_decay,\n",
    "#     final_epsilon=final_epsilon,\n",
    "#     env_stat=env_stat,)\n",
    "#         agent_MC=constant_learning_rate_MC(\n",
    "#     n_episodes=n_episodes,\n",
    "#     learning_rate=learning_rate,\n",
    "#     discount_factor=discount_factor,\n",
    "#     initial_epsilon=start_epsilon,\n",
    "#     epsilon_decay=epsilon_decay,\n",
    "#     final_epsilon=final_epsilon,\n",
    "#     env_stat=env_stat,)\n",
    "\n",
    "#         agent_TD.train()\n",
    "#         agent_TD.plot(1000)\n",
    "\n",
    "\n",
    "#         agent_sarsa.train()\n",
    "#         agent_sarsa.plot(1000)\n",
    "\n",
    "\n",
    "#         agent_E_sarsa.train()\n",
    "#         agent_E_sarsa.plot(1000)\n",
    "\n",
    "\n",
    "#     agent_MC.train()\n",
    "#     agent_MC.plot(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = [0.5, 0.8, 0.9, 0.95, 0.99]\n",
    "# discount_factors = [0.8, 0.95, 0.99]\n",
    "# for learning_rate in learning_rates:\n",
    "#     for discount_factor in discount_factors:\n",
    "#         agent_TD=TD_zero_for_Qvalue(\n",
    "#     n_episodes=n_episodes,\n",
    "#     learning_rate=learning_rate,\n",
    "#     discount_factor=discount_factor,\n",
    "#     initial_epsilon=start_epsilon,\n",
    "#     epsilon_decay=epsilon_decay,\n",
    "#     final_epsilon=final_epsilon,\n",
    "#     env_stat=env_stat,)\n",
    "#         agent_sarsa=SARSA(\n",
    "#     n_episodes=n_episodes,\n",
    "#     learning_rate=learning_rate,\n",
    "#     discount_factor=discount_factor,\n",
    "#     initial_epsilon=start_epsilon,\n",
    "#     epsilon_decay=epsilon_decay,\n",
    "#     final_epsilon=final_epsilon,\n",
    "#     env_stat=env_stat,)\n",
    "#         agent_E_sarsa=Expected_SARSA(\n",
    "#     n_episodes=n_episodes,\n",
    "#     learning_rate=learning_rate,\n",
    "#     discount_factor=discount_factor,\n",
    "#     initial_epsilon=start_epsilon,\n",
    "#     epsilon_decay=epsilon_decay,\n",
    "#     final_epsilon=final_epsilon,\n",
    "#     env_stat=env_stat,)\n",
    "#         agent_MC=constant_learning_rate_MC(\n",
    "#     n_episodes=n_episodes,\n",
    "#     learning_rate=learning_rate,\n",
    "#     discount_factor=discount_factor,\n",
    "#     initial_epsilon=start_epsilon,\n",
    "#     epsilon_decay=epsilon_decay,\n",
    "#     final_epsilon=final_epsilon,\n",
    "#     env_stat=env_stat,)\n",
    "\n",
    "#         agent_TD.train()\n",
    "#         agent_TD.plot(1000)\n",
    "\n",
    "\n",
    "#         agent_sarsa.train()\n",
    "#         agent_sarsa.plot(1000)\n",
    "\n",
    "\n",
    "#         agent_E_sarsa.train()\n",
    "#         agent_E_sarsa.plot(1000)\n",
    "\n",
    "\n",
    "#     agent_MC.train()\n",
    "#     agent_MC.plot(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 35382/50000 [04:30<01:51, 131.21it/s]"
     ]
    }
   ],
   "source": [
    "agent_TD=TD_zero_for_Qvalue(\n",
    "    n_episodes=50000,\n",
    "    learning_rate=0.95,\n",
    "    discount_factor=0.99,\n",
    "    initial_epsilon=1,\n",
    "    epsilon_decay=1/50000,\n",
    "    final_epsilon=0.001,\n",
    "    env_stat=env_stat)\n",
    "\n",
    "agent_E_sarsa=Expected_SARSA(\n",
    "    n_episodes=n_episodes,\n",
    "    learning_rate=0.2,\n",
    "    discount_factor=discount_factor,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    env_stat=env_stat,)\n",
    "\n",
    "#agent_TD.train()\n",
    "#agent_TD.plot(1000)\n",
    "\n",
    "\n",
    "#agent_sarsa.train()\n",
    "#agent_sarsa.plot(1000)\n",
    "\n",
    "\n",
    "agent_E_sarsa.train()\n",
    "agent_E_sarsa.plot(1000)\n",
    "\n",
    "\n",
    "#agent_MC.train()\n",
    "#agent_MC.plot(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Frozen lake\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# obs, info = env_stat.reset()\n",
    "# done = False\n",
    "# terminated=False\n",
    "# truncated=False\n",
    "# for i in range(20):\n",
    "#     done= terminated or truncated\n",
    "#     if done:\n",
    "#         obs, info = env_stat.reset()\n",
    "#     img=env_stat.render()\n",
    "#     ax.imshow(img)\n",
    "#     action = agent_TD.get_action(obs)\n",
    "#     if action==0:\n",
    "#         title= \"TD policy action: left\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==1:\n",
    "#         title=\"TD policy action: down\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==2:\n",
    "#         title=\"TD policy action: right\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==3:\n",
    "#         title= \"TD policy action: up\"\n",
    "#         ax.set_title(title)\n",
    "#     next_obs, reward, terminated, truncated, info = env_stat.step(action)\n",
    "#     obs=next_obs\n",
    "#     plt.savefig(\"TD_lake-\"+str(i))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Cliff\n",
    "# #[0,0], [0,1], ... ,[0,11]\n",
    "# #[1,0]\n",
    "# #\n",
    "# #[3,0], [3,1], ... ,[3,11]\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# obs, info = env_stat.reset()\n",
    "# done = False\n",
    "# terminated=False\n",
    "# truncated=False\n",
    "# for i in range(20):\n",
    "#     done= terminated or truncated\n",
    "#     if done:\n",
    "#         obs, info = env_stat.reset()\n",
    "#     img=env_stat.render()\n",
    "#     ax.imshow(img)\n",
    "#     action = agent_TD.get_action(obs)\n",
    "#     if action==0:\n",
    "#         title= \"TD policy action: up\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==1:\n",
    "#         title=\"TD policy action: right\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==2:\n",
    "#         title=\"TD policy action: down\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==3:\n",
    "#         title= \"TD policy action: left\"\n",
    "#         ax.set_title(title)\n",
    "#     next_obs, reward, terminated, truncated, info = env_stat.step(action)\n",
    "#     obs=next_obs\n",
    "#     plt.savefig(\"TD_cliff-\"+str(i))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Taxi\n",
    "# # #Obs space: ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# obs, info = env_stat.reset()\n",
    "# done = False\n",
    "# terminated=False\n",
    "# truncated=False\n",
    "# for i in range(20):\n",
    "#     done= terminated or truncated\n",
    "#     if done:\n",
    "#         obs, info = env_stat.reset()\n",
    "#     img=env_stat.render()\n",
    "#     ax.imshow(img)\n",
    "#     action = agent_TD.get_action(obs)\n",
    "#     if action==0:\n",
    "#         title= \"TD policy action: down\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==1:\n",
    "#         title=\"TD policy action: up\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==2:\n",
    "#         title=\"TD policy action: right\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==3:\n",
    "#         title= \"TD policy action: left\"\n",
    "#     ax.set_title(title)\n",
    "#     if action==4:\n",
    "#         title= \"TD policy action: pickup passenger\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==5:\n",
    "#         title= \"TD policy action: drop off passenger\"\n",
    "#         ax.set_title(title)\n",
    "#     next_obs, reward, terminated, truncated, info = env_stat.step(action)\n",
    "#     obs=next_obs\n",
    "#     plt.savefig(\"TD_taxi-\"+str(i))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Blackjack\n",
    "# #Obs space: (player sum, dealer showing card, usable ace (always 11 unless busts))\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# obs, info = env_stat.reset()\n",
    "# done = False\n",
    "# terminated=False\n",
    "# truncated=False\n",
    "# for i in range(20):\n",
    "#     done= terminated or truncated\n",
    "#     if done:\n",
    "#         obs, info = env_stat.reset()\n",
    "#     img=env_stat.render()\n",
    "#     ax.imshow(img)\n",
    "#     action = agent_TD.get_action(obs)\n",
    "#     if action==0:\n",
    "#         title= \"TD policy action: stick\"\n",
    "#         ax.set_title(title)\n",
    "#     if action==1:\n",
    "#         title=\"TD policy action: hit\"\n",
    "#         ax.set_title(title)\n",
    "#     next_obs, reward, terminated, truncated, info = env_stat.step(action)\n",
    "#     obs=next_obs\n",
    "#     plt.savefig(\"TD_blackjack-\"+str(i))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
